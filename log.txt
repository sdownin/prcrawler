INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 747f49385d8d280d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6023
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 732b64b7e0b9a07e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6024
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 27c500a48fd7b1e0
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6025
DEBUG: Crawled (200) <GET https://www.jnj.com/media-center/press-releases> (referer: None)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/press-release/press-releases-archive> (referer: None)
DEBUG: Filtered offsite request to 'twitter.com': <GET https://twitter.com/jnjnews>
DEBUG: Filtered offsite request to 'www.youtube.com': <GET http://www.youtube.com/user/pfizernews?feature=results_main>
DEBUG: Filtered offsite request to 'twitter.com': <GET http://twitter.com/pfizer_news>
DEBUG: Redirecting (301) to <GET https://investors.pfizer.com/why-invest-our-story/default.aspx> from <GET http://investors.pfizer.com/why-invest-our-story/default.aspx>
DEBUG: Crawled (200) <GET https://ourstory.jnj.com/> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.careers.jnj.com/career-stories/maria-duda-kertesz> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Retrying <GET https://www.novartis.com/news/news-archive> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 1c58c62fd07b0b96
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6023
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 6e03f0c0f0383cf3
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6024
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: e69f111e232122fd
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO: Enabled item pipelines:
['prcrawler.pipelines.MongoPipeline']
INFO: Spider opened
INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG: Telnet console listening on 127.0.0.1:6025
DEBUG: Crawled (200) <GET https://www.novartis.com/news/news-archive> (referer: None)
DEBUG: Crawled (200) <GET https://www.jnj.com/media-center/press-releases> (referer: None)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/press-release/press-releases-archive> (referer: None)
DEBUG: Filtered duplicate request: <GET https://www.novartis.com/news/news-archive#content> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG: Filtered offsite request to 'www.cnnmoney.ch': <GET https://www.cnnmoney.ch/shows/newsmaker/videos/steven-baert-chro-novartis-flipping-pyramid>
DEBUG: Filtered offsite request to 'twitter.com': <GET https://twitter.com/jnjnews>
DEBUG: Filtered offsite request to 'www.youtube.com': <GET http://www.youtube.com/user/pfizernews?feature=results_main>
DEBUG: Filtered offsite request to 'twitter.com': <GET http://twitter.com/pfizer_news>
DEBUG: Crawled (200) <GET https://ourstory.jnj.com/> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Redirecting (301) to <GET https://investors.pfizer.com/why-invest-our-story/default.aspx> from <GET http://investors.pfizer.com/why-invest-our-story/default.aspx>
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.careers.jnj.com/career-stories/maria-duda-kertesz> (referer: https://www.jnj.com/media-center/press-releases)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.novartis.com/news/news-archive#navigation> (referer: https://www.novartis.com/news/news-archive)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.jnj.com/latest-news> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.novartis.com/news/news-archive?page=1> (referer: https://www.novartis.com/news/news-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.jnj.com/media-center/press-releases> (referer: https://www.jnj.com/media-center/press-releases)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0> (referer: https://www.novartis.com/news/news-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.jnj.com/media-center/press-releases?p=2> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Retrying <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/for_pfizer_s_gertjan_ophorst_completing_the_new_york_city_marathon_was_a_dream_come_true> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Crawled (200) <GET https://www.pfizer.com/people/history> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/>

None
DEBUG: Crawled (200) <GET https://www.novartis.com/news/news-archive?type=manually_created_external_links> (referer: https://www.novartis.com/news/news-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Crawled 4 pages (at 4 pages/min), scraped 7 items (at 7 items/min)
INFO: Crawled 5 pages (at 5 pages/min), scraped 9 items (at 9 items/min)
INFO: Crawled 6 pages (at 6 pages/min), scraped 25 items (at 25 items/min)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
DEBUG: Crawled (200) <GET https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
DEBUG: Retrying <GET https://investors.pfizer.com/why-invest-our-story/default.aspx> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
DEBUG: Filtered duplicate request: <GET https://ourstory.jnj.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.careers.jnj.com/career-stories/maria-duda-kertesz>

None
DEBUG: Crawled (200) <GET https://ourstory.jnj.com/cookiepolicy> (referer: https://ourstory.jnj.com/)
DEBUG: Crawled (200) <GET https://www.jnj.com/janssen-announces-u-s-fda-approval-of-spravatotm-esketamine-ciii-nasal-spray-for-adults-with-treatment-resistant-depression-trd-who-have-cycled-through-multiple-treatments-without-relief> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.novartis.com/news/media-releases/novartis-data-confirm-rapid-response-and-high-efficacy-cosentyx-psoriasis-patients-first-time-china> (referer: https://www.novartis.com/news/news-archive)
DEBUG: Retrying <GET https://www.novartis.com/stories/access-healthcare/novartis-access-impact-evaluation-progress-update-our-work-boston-university> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/shining_a_light_on_the_impact_of_rare_diseases> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Crawled 6 pages (at 2 pages/min), scraped 18 items (at 11 items/min)
INFO: Crawled 6 pages (at 1 pages/min), scraped 21 items (at 12 items/min)
INFO: Crawled 9 pages (at 3 pages/min), scraped 38 items (at 13 items/min)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Retrying <GET https://www.jnj.com/janssen-announces-u-s-fda-approval-of-novel-tremfya-guselkumab-one-press-patient-controlled-injector-for-adults-with-moderate-to-severe-plaque-psoriasis> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.pfizer.com/news/featured_stories/featured_stories_detail/turning_the_promise_of_gene_therapy_into_a_reality> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Filtered offsite request to 'www.facebook.com': <GET https://www.facebook.com/dialog/share?app_id=217277672020596&display=popup&href=https://www.careers.jnj.com/career-stories/maria-duda-kertesz>
DEBUG: Filtered offsite request to 'www.linkedin.com': <GET https://www.linkedin.com/shareArticle?url=https://www.careers.jnj.com/career-stories/maria-duda-kertesz&mini=true&title=Career%20Milestones%3A%20Maria%20%E2%80%9CDuda%E2%80%9D%20Kertesz&source=J%26J%20Careers>
DEBUG: Crawled (200) <GET https://ourstory.jnj.com/termsofuse> (referer: https://ourstory.jnj.com/)
DEBUG: Crawled (200) <GET https://www.jnj.com/media-center> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.novartis.com/news/news-archive?type=story> (referer: https://www.novartis.com/news/news-archive)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/press-kits/media-contacts> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/cookiepolicy>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/janssen-announces-u-s-fda-approval-of-spravatotm-esketamine-ciii-nasal-spray-for-adults-with-treatment-resistant-depression-trd-who-have-cycled-through-multiple-treatments-without-relief>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-data-confirm-rapid-response-and-high-efficacy-cosentyx-psoriasis-patients-first-time-china>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/shining_a_light_on_the_impact_of_rare_diseases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Retrying <GET https://ourstory.jnj.com/privacy> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.novartis.com/news/media-releases/novartis-cosentyx-shows-superior-improvements-psoriasis-patients-quality-life-versus-janssens-il-23-stelara> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.pfizer.com/news/press-kits/ra-narrative> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/cookiepolicy>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/janssen-announces-u-s-fda-approval-of-spravatotm-esketamine-ciii-nasal-spray-for-adults-with-treatment-resistant-depression-trd-who-have-cycled-through-multiple-treatments-without-relief>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-data-confirm-rapid-response-and-high-efficacy-cosentyx-psoriasis-patients-first-time-china>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/shining_a_light_on_the_impact_of_rare_diseases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Crawled 7 pages (at 1 pages/min), scraped 31 items (at 13 items/min)
INFO: Crawled 7 pages (at 1 pages/min), scraped 34 items (at 13 items/min)
INFO: Crawled 11 pages (at 2 pages/min), scraped 54 items (at 16 items/min)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Crawled (200) <GET https://ourstory.jnj.com/about> (referer: https://ourstory.jnj.com/)
DEBUG: Retrying <GET https://investors.pfizer.com/why-invest-our-story/default.aspx> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Crawled (200) <GET https://www.jnj.com/janssen-announces-u-s-fda-approval-of-novel-tremfya-guselkumab-one-press-patient-controlled-injector-for-adults-with-moderate-to-severe-plaque-psoriasis> (referer: https://www.jnj.com/media-center/press-releases)
DEBUG: Crawled (200) <GET https://www.novartis.com/news/media-releases/three-winners-2019-sandoz-healthcare-access-challenge-hack-are-announced-sxsw> (referer: https://www.novartis.com/news/news-archive)
DEBUG: Crawled (200) <GET https://www.careers.jnj.com/career-stories/rebeca-lugo> (referer: https://www.careers.jnj.com/career-stories/maria-duda-kertesz)
DEBUG: Crawled (200) <GET https://www.pfizer.com/news/press-kits/oncology> (referer: https://www.pfizer.com/news/press-release/press-releases-archive)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/termsofuse>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=story>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/press-kits/media-contacts>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/cookiepolicy>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/janssen-announces-u-s-fda-approval-of-spravatotm-esketamine-ciii-nasal-spray-for-adults-with-treatment-resistant-depression-trd-who-have-cycled-through-multiple-treatments-without-relief>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-data-confirm-rapid-response-and-high-efficacy-cosentyx-psoriasis-patients-first-time-china>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/shining_a_light_on_the_impact_of_rare_diseases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Retrying <GET https://www.jnj.com/personal-stories> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.pfizer.com/news/press-kits/frequently-requested-info> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://www.novartis.com/stories/discovery/enabling-130000-employees-grow-organization-committed-continuous-learning> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
DEBUG: Retrying <GET https://ourstory.jnj.com/contact> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
CRITICAL: Unhandled Error
Traceback (most recent call last):
  File "run_industry_prcrawler.py", line 56, in <module>
    run()
  File "run_industry_prcrawler.py", line 52, in run
    reactor.run()
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 1261, in run
    self.mainLoop()
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 1270, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 869, in runUntilCurrent
    f(*a, **kw)
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.

INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/termsofuse>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=story>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/press-kits/media-contacts>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://ourstory.jnj.com/cookiepolicy>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/janssen-announces-u-s-fda-approval-of-spravatotm-esketamine-ciii-nasal-spray-for-adults-with-treatment-resistant-depression-trd-who-have-cycled-through-multiple-treatments-without-relief>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-data-confirm-rapid-response-and-high-efficacy-cosentyx-psoriasis-patients-first-time-china>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/shining_a_light_on_the_impact_of_rare_diseases>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/johnson-johnson-innovation-names-winners-of-inaugural-champions-of-science-africa-storytelling-challenge>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/new_rna_technology_could_get_the_flu_vaccine_right_every_year>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases?p=2>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/people/history>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?type=manually_created_external_links>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/media-center/press-releases>

None
INFO: Crawled 8 pages (at 1 pages/min), scraped 43 items (at 12 items/min)
INFO: Crawled 8 pages (at 1 pages/min), scraped 45 items (at 11 items/min)
INFO: Crawled 14 pages (at 3 pages/min), scraped 69 items (at 15 items/min)
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/media-releases/novartis-shareholders-approve-all-resolutions-proposed-board-directors-annual-general-meeting-0>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.jnj.com/latest-news>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive?page=1>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/my_sister_s_breast_cancer_diagnosis_and_what_it_meant_for_me>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.novartis.com/news/news-archive>

None
INFO: Item added to MongoDB!
DEBUG: Scraped from <200 https://www.pfizer.com/news/featured_stories/featured_stories_detail/more_at_risk_children_in_developing_countries_will_be_protected_against_pneumococcal_disease_thanks_to_new_and_expanded_partnerships>

None
DEBUG: Retrying <GET https://www.careers.jnj.com/career-stories/matthew-simkovic> (failed 1 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.
].
DEBUG: Retrying <GET https://ourstory.jnj.com/archives> (failed 1 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.
].
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: d46efac6d624017f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
WARNING: C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py:25: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 25 of the file C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py. To get rid of this warning, pass the additional argument 'features="lxml"' to the BeautifulSoup constructor.

  bsoup = BeautifulSoup(r.text)

INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 878eeffc323fe1ab
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8ea7e4b1819a4f03
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 3245c46c8ebb61f9
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 0110c5fa17ef4b3d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c21b6fb5c33edb52
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c0a0ba8b28451bf5
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 852337d91649b242
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 72a78c3171a92ae3
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: f8e1ed59b6feb826
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 3c692a9e460f2766
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: bea12fadfaa94052
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: b98a529dbccecfca
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: ab9d1fecb565db90
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 64cb717d2a8de049
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 26319b996904c895
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c36e2dd3d7e08faa
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 4927c9d92f0e8e7c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'ford.com': No schema supplied. Perhaps you meant http://ford.com?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 2a4f3d391be3d4b0
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 0ee9a6a11f1f0b13
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 76640b2cf5479a91
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: babdde66a0cfc7f6
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 6ebd155368e00ec8
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 127b1d623065f67f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 798207d443f64ec9
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 1f8c0cbd1fccc470
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 926fc21daa3a139a
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: b6474d30aa11ddd6
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 6e5a55898b4cf3e2
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 438194b892698d3d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 7db44575c881f428
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 0dd41c86ee16e63a
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 64, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: abb175330e880d37
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
WARNING: C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py:25: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 25 of the file C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py. To get rid of this warning, pass the additional argument 'features="lxml"' to the BeautifulSoup constructor.

  bsoup = BeautifulSoup(r.text)

INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 2a6d622f59372b70
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 3dcb3fbfe5dcc3e9
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 77c0fde3fe85ea7c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: ba59d481c5951872
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 5384a615014c1085
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: d4e7c06e25e25190
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8caa23f1e54c736b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 142aa46254151219
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 7e648267960bff0a
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 96cfeef0b5a011b6
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8db0f43b91956c7b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8b182da5e12da027
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 092ed6d93e5d541e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 26b4cf6ed0e8095f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 87231cb23e4cfaf1
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 1ae7d0f3b8038490
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 5ef7246588c4b2c4
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'ford.com': No schema supplied. Perhaps you meant http://ford.com?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 09b74e163f576dc4
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: eefaf7d0ca7907fa
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 62a2125d2329d2f5
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: ffa6c7310ac75ead
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 370d948b885ee1b5
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: e271de5dba0a5b7f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8b1290fe93e821a8
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c473781eeacd3f0e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 6283628b8abcd5cf
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 43fac5b11f2367c1
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: e09149c9322c85f6
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 5b0a4b20bf4bf149
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 0b5dda50e15ac23d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 05d839dfaec538e5
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8f8927a989d86242
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
WARNING: C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py:25: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 25 of the file C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py. To get rid of this warning, pass the additional argument 'features="lxml"' to the BeautifulSoup constructor.

  bsoup = BeautifulSoup(r.text)

INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 6b7a9f8051dac6e0
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 35fff866c5cae0f7
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 2c266a9e99a3c175
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 9f0b169bc0d194d9
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 76cbecc32d37c638
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 592cfd00f33400fe
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: f4923babc7179c2a
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 376d7f18864ed382
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c23c96ab1e659132
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: d79bc88b93f7434c
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 76dbbfc23d184b20
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 4e9a327a71e20d5f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 81f43f37c7ae14e2
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 062d12696bd53e25
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 3c96f1585ee06cb2
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8b26f871f5056892
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: c2897b57f55e891b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'ford.com': No schema supplied. Perhaps you meant http://ford.com?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 49985b815ef43864
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 19ba8fff0e22ecc0
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: d780b72380ba7081
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 5e4da7e20cebcf8f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 9b31b3022323cb7d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: a7c0cd86cc77108e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 50, in __init__
    exit()
  File "C:\Anaconda3\lib\_sitebuiltins.py", line 26, in __call__
    raise SystemExit(code)
SystemExit: None
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 5a756db9207c0dfa
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: b1778c8c61038d78
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 96d495ac1abca96d
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 37bf53417bf6a23f
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: a89469d3ce6e4be0
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: 8baac612a7ae6c3b
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: d9e79b03bd66144e
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
INFO: Overridden settings: {'BOT_NAME': 'prcrawler', 'CONCURRENT_REQUESTS_PER_IP': 8, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 5, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'NEWSPIDER_MODULE': 'prcrawler.spiders', 'SPIDER_MODULES': ['prcrawler.spiders'], 'USER_AGENT': ('Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',)}
INFO: Telnet Password: ee2ff20d44430f62
INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 79, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\crawler.py", line 102, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\crawl.py", line 100, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "C:\Anaconda3\lib\site-packages\scrapy\spiders\__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
  File "C:\Users\T430\prcrawler\prcrawler\spiders\prspider.py", line 24, in __init__
    r = requests.get(params['start_url'])
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 498, in request
    prep = self.prepare_request(req)
  File "C:\Anaconda3\lib\site-packages\requests\sessions.py", line 441, in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 309, in prepare
    self.prepare_url(url, params)
  File "C:\Anaconda3\lib\site-packages\requests\models.py", line 383, in prepare_url
    raise MissingSchema(error)
requests.exceptions.MissingSchema: Invalid URL 'nan': No schema supplied. Perhaps you meant http://nan?
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
CRITICAL: Unhandled error in Deferred:
CRITICAL: 
Traceback (most recent call last):
  File "C:\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "run_industry_prcrawler.py", line 66, in <lambda>
    d.addBoth(lambda _: reactor.stop())
  File "C:\Anaconda3\lib\site-packages\twisted\internet\base.py", line 630, in stop
    "Can't stop reactor that isn't running.")
twisted.internet.error.ReactorNotRunning: Can't stop reactor that isn't running.
